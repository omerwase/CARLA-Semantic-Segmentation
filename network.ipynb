{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Network with Dilated Convolutions\n",
    "\n",
    "### Final Submission for Lyft's Preception Challenge\n",
    "\n",
    "#### Results from final submitted run (after retraining):\n",
    "    MODEL_NAME: dcn_v5\n",
    "    MODEL_SAVE_VER: 00_r2\n",
    "    TRAIN_DIR: /home/ow/Documents/udacity/lyft/datasets/combined_v05/train_v4\n",
    "    TEST_DIR: /home/ow/Documents/udacity/lyft/datasets/combined_v05/test_v6\n",
    "    train_images.shape: (6400, 408, 800, 3)\n",
    "    train_labels.shape: (6400, 408, 800, 10)\n",
    "    test_images.shape: (500, 408, 800, 3)\n",
    "    test_labels.shape: (500, 600, 800, 10)\n",
    "\n",
    "    Training epoch: 12/200\n",
    "    Training time: 767.930s, loss: 0.01681\n",
    "    Prediction session time: 16.900s\n",
    "    F1 scores: Back   Vehi   Road   Fence  Ped    Poles  Side   Veg    BW     OT      \n",
    "               0.9549 0.7994 0.9924 0.7919 0.7320 0.7993 0.9660 0.8505 0.9090 0.7948\n",
    "    prec_v: 0.71346, recall_v: 0.90887\n",
    "    prec_r: 0.99116, recall_r: 0.99367\n",
    "    fscore_avg: 0.92666, fscore_v: 0.86167, fscore_r: 0.99166\n",
    "    Total time: 833.581s\n",
    "    *************** MODEL SAVED ON SCORE ***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import helper_functions as hf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "from functools import reduce\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperarameters and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'dcn_v5'\n",
    "MODEL_SAVE_VER = '00'\n",
    "SAVE_EPSILON = 1e-4\n",
    "\n",
    "EPOCHS = 200\n",
    "SHUFFLE_PER_EPOCH = True\n",
    "BATCH_SIZE = 12\n",
    "L2_REG = 1e-5\n",
    "STD_DEV = 1e-2\n",
    "LEARNING_RATE = 1e-4\n",
    "KEEP_PROB = 0.5 \n",
    "EPSILON = 1e-6\n",
    "ADAM_EPSILON = 1e-6\n",
    "\n",
    "TRIM_IND = (121, 497) # Triming is always applied\n",
    "FLIP = True # Images randomly flipped (horizontal) during training\n",
    "RESHAPE = False # If images should be reshaped\n",
    "PREPROCESS = True # If images should be preprocessed\n",
    "\n",
    "# Consolidated labels used to improve inference speed and reduce memory footprint\n",
    "NEW_LABELS = True # New labels of 20 (Building + Wall) and 30 (Other + Traffic Sign)\n",
    "LABEL_CHANNELS = [10, 7, 2, 4, 5, 8, 9, 20, 30]\n",
    "CHANNEL_NAMES = ['Back', 'Vehi', 'Road', 'Fence', 'Ped', 'Poles', 'Side', 'Veg', 'BW', 'OT']\n",
    "LOSS_WEIGHTS = [0.3, 1.2, 0.4, 0.3, 1.0, 0.5, 0.3, 0.3, 0.3, 0.5]\n",
    "\n",
    "NUM_CLASSES = len(LABEL_CHANNELS) + 1\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'datasets', 'combined_v05')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train_v4')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test_v6')\n",
    "SAVE_DIR = os.path.join(os.getcwd(), 'saved_models', MODEL_NAME, MODEL_SAVE_VER)\n",
    "WEIGHTS_FILE = os.path.join(os.getcwd(), 'weights', 'pretrained_weights.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(image_input, keep_prob, weights_file, num_classes):   \n",
    "    \"\"\"\n",
    "    Builds custom network based on VGG:\n",
    "    1) First 13 layers (10 conv, 3 dialated conv) use pre-trained weights\n",
    "    2) Remaining layers are new, resized with reduced depth than original VGG layers\n",
    "    3) Final layer is added with scaled (0.01) output from conv3 layer\n",
    "    \n",
    "    \n",
    "    :param image_input: image input tensor\n",
    "    :param keep_prob: placeholder for drop out\n",
    "    :weights_file: path to pre-trained weights file\n",
    "    :num_classes: number of classes for final output\n",
    "    :return: logits, prediction, one_hot\n",
    "    \"\"\"    \n",
    "    def conv_layer(name, input_layer, weights):\n",
    "        \"\"\"\n",
    "        Builds conv layers from pre-trained weights\n",
    "        Adopted from: \n",
    "            1) https://github.com/fyu/dilation\n",
    "            2) https://github.com/ndrplz/dilation-tensorflow\n",
    "            \n",
    "        :param name: layer name\n",
    "        :param input_layer: input tensor\n",
    "        :param weights: pre-trained weights dictionary\n",
    "        :return: conv layer tensor\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            kernel = tf.Variable(initial_value=weights[name[:7] + '/kernel:0'], name='kernel')\n",
    "            bias = tf.Variable(initial_value=weights[name[:7] + '/bias:0'], name='bias')\n",
    "            conv = tf.nn.conv2d(input_layer, kernel, strides=[1,1,1,1], padding='SAME', name='conv')\n",
    "            out = tf.nn.bias_add(conv, bias, name='bias_add')\n",
    "            out = tf.nn.relu(out, name='relu')\n",
    "            return out\n",
    "\n",
    "    def aconv_layer(name, input_layer, weights, rate):\n",
    "        \"\"\"\n",
    "        Builds atrous/dilated conv layers from pre-trained weights\n",
    "        Adopted from: \n",
    "            1) https://github.com/fyu/dilation\n",
    "            2) https://github.com/ndrplz/dilation-tensorflow\n",
    "            \n",
    "        :param name: layer name\n",
    "        :param input_layer: input tensor\n",
    "        :param weights: pre-trained weights dictionary\n",
    "        :param rate: rate of dilation\n",
    "        :return: atrous/dilated conv layer tensor\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            kernel = tf.Variable(initial_value=weights[name[1:8] + '/kernel:0'], name='kernel')\n",
    "            bias = tf.Variable(initial_value=weights[name[1:8] + '/bias:0'], name='bias')\n",
    "            aconv = tf.nn.atrous_conv2d(input_layer, kernel, rate, padding='SAME', name='aconv')\n",
    "            out = tf.nn.bias_add(aconv, bias, name='bias_add')\n",
    "            out = tf.nn.relu(out, name='relu')\n",
    "            return out\n",
    "\n",
    "    def max_pool(name, input_layer):\n",
    "        \"\"\"\n",
    "        Builds maxpooling layer with VGG default values\n",
    "        \n",
    "        :param name: layer name\n",
    "        :param input_layer: input tensor\n",
    "        :return: maxpooling layer tensor\n",
    "        \"\"\"\n",
    "        return tf.layers.max_pooling2d(input_layer, pool_size=(2,2), strides=(2,2), padding='SAME', name=name)\n",
    "\n",
    "    with open(weights_file, 'rb') as f:\n",
    "        pre_w = pickle.load(f)\n",
    "    \n",
    "    conv1_1 = conv_layer('conv1_1_64', image_input, pre_w)\n",
    "    conv1_2 = conv_layer('conv1_2_64', conv1_1, pre_w)\n",
    "    pool1 = max_pool('pool1', conv1_2)\n",
    "    \n",
    "    conv2_1 = conv_layer('conv2_1_128', pool1, pre_w)\n",
    "    conv2_2 = conv_layer('conv2_2_128', conv2_1, pre_w)\n",
    "    pool2 = max_pool('pool2', conv2_2)\n",
    "    \n",
    "    conv3_1 = conv_layer('conv3_1_256', pool2, pre_w)\n",
    "    conv3_2 = conv_layer('conv3_2_256', conv3_1, pre_w)\n",
    "    conv3_3 = conv_layer('conv3_3_256', conv3_2, pre_w)\n",
    "    pool3 = max_pool('pool3', conv3_3)\n",
    "    \n",
    "    conv4_1 = conv_layer('conv4_1_512', pool3, pre_w)\n",
    "    conv4_2 = conv_layer('conv4_2_512', conv4_1, pre_w)\n",
    "    conv4_3 = conv_layer('conv4_3_512', conv4_2, pre_w)\n",
    "    \n",
    "    # Dilated Convolutions, rate = 2\n",
    "    conv5_1 = aconv_layer('dconv5_1_512', conv4_3, pre_w, 2)\n",
    "    conv5_2 = aconv_layer('dconv5_2_512', conv5_1, pre_w, 2)\n",
    "    conv5_3 = aconv_layer('dconv5_3_512', conv5_2, pre_w, 2)\n",
    "    \n",
    "    # Dialated Convolition, rate = 4\n",
    "    dconv6_1 = tf.layers.conv2d(conv5_3, 512, kernel_size=7, strides=1, padding='SAME', \n",
    "                           name='dconv6_1_512',\n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG),\n",
    "                           dilation_rate=4, activation=tf.nn.relu)\n",
    "    \n",
    "    drop1 = tf.nn.dropout(dconv6_1, keep_prob, name='drop1') \n",
    "    \n",
    "    conv7_1_512 = tf.layers.conv2d(drop1, 512, kernel_size=1, strides=1, padding='SAME', \n",
    "                           name='conv7_1_512',\n",
    "                           kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG),\n",
    "                           activation=tf.nn.relu)\n",
    "    \n",
    "    drop2 = tf.nn.dropout(conv7_1_512, keep_prob, name='drop2')\n",
    "\n",
    "    \n",
    "    conv8_1_10 = tf.layers.conv2d(drop2, num_classes, kernel_size=1, strides=1, padding='SAME', \n",
    "                               name='conv8_1_10',\n",
    "                               kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG))   \n",
    "    \n",
    "    conv8_upsample = tf.layers.conv2d_transpose(conv8_1_10, num_classes, 4, 2, padding='SAME', \n",
    "                                        name='conv8_up_10',\n",
    "                                        kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG)) \n",
    "    \n",
    "    conv3_4_10 = tf.layers.conv2d(conv3_3, num_classes, kernel_size=1, strides=1, padding='SAME', \n",
    "                                 name='conv3_4_10',\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG))\n",
    "    \n",
    "    conv3_scale = tf.multiply(conv3_4_10, 0.01, name='conv3_scale')\n",
    "    \n",
    "    # Combining final output with output from conv3 layer\n",
    "    conv3_conv8_add = tf.add(conv8_upsample, conv3_scale, name='conv3_conv8_add')\n",
    "    \n",
    "    with tf.name_scope('output'):\n",
    "        logits = tf.layers.conv2d_transpose(conv3_conv8_add, num_classes, 8, 4, \n",
    "                                    padding='SAME', name='logits',\n",
    "                                    kernel_initializer=tf.truncated_normal_initializer(stddev=STD_DEV),\n",
    "                                    kernel_regularizer=tf.contrib.layers.l2_regularizer(L2_REG)) \n",
    "        softmax = tf.nn.softmax(logits, name='softmax')\n",
    "        prediction = tf.argmax(softmax, axis=3, name='prediction')\n",
    "        one_hot = tf.one_hot(prediction, depth=num_classes, dtype=tf.uint8, name='one_hot')\n",
    "        \n",
    "    return logits, prediction, one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(logits, labels, l_rate, adam_eps, weights=None):\n",
    "    \"\"\"\n",
    "    Creates optimization and loss functions:\n",
    "        1) Uses Adam optimizer\n",
    "        2) Loss based on weighted cross entropy + regularization loss\n",
    "    \n",
    "    :param logits: logits tensor from network()\n",
    "    :param labels: placeholder for training labels\n",
    "    :param l_rate: placeholder for learning rate value\n",
    "    :param adam_eps: placeholder for Adam epsilon values\n",
    "    :param weights: placeholder for weights, if None no weighting is applied\n",
    "    :return: optimizer, total_loss\n",
    "    \"\"\"\n",
    "    with tf.name_scope('optimize'):\n",
    "        logits = tf.reshape(logits, (-1, NUM_CLASSES))\n",
    "        labels = tf.to_float(tf.reshape(labels, (-1, NUM_CLASSES)))\n",
    "        \n",
    "        if weights is not None:\n",
    "            softmax = tf.nn.softmax(logits) + EPSILON\n",
    "            cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax), weights),\n",
    "                                           reduction_indices=[1])\n",
    "        else:\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
    "            \n",
    "        cross_entropy_loss = tf.reduce_mean(cross_entropy,\n",
    "                                            name='xent_mean_loss')\n",
    "                                        \n",
    "        reg_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES),\n",
    "                            name='reg_loss')\n",
    "        total_loss = tf.add_n([cross_entropy_loss, reg_loss], name='total_loss')\n",
    "        \n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=l_rate, epsilon=adam_eps).minimize(total_loss)\n",
    "        \n",
    "    return optimizer, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    print(f'MODEL_NAME: {MODEL_NAME}')\n",
    "    print(f'MODEL_SAVE_VER: {MODEL_SAVE_VER}')\n",
    "    print(f'TRAIN_DIR: {TRAIN_DIR}')\n",
    "    print(f'TEST_DIR: {TEST_DIR}')\n",
    "    \n",
    "    get_train_batch = hf.train_batch_gen(TRAIN_DIR, LABEL_CHANNELS, reshape=RESHAPE, \n",
    "                                         preprocess=PREPROCESS, new_labels=NEW_LABELS, \n",
    "                                         trim_ind=TRIM_IND)\n",
    "    get_test_batch, revert_trim_reshape = hf.test_batch_gen(TEST_DIR, LABEL_CHANNELS, \n",
    "                                          reshape=RESHAPE, preprocess=PREPROCESS, new_labels=NEW_LABELS,\n",
    "                                          trim_ind=TRIM_IND)\n",
    "    \n",
    "    \n",
    "    # Images loaded into memory for faster training\n",
    "    # Created swapfiles to increase memory\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    train_names = []\n",
    "    for images, labels, names in get_train_batch(1):\n",
    "        train_images.append(images)\n",
    "        train_labels.append(labels)\n",
    "        train_names += names\n",
    "\n",
    "    train_images = np.array(train_images, dtype=np.uint8)\n",
    "    train_images = train_images.reshape(-1, *train_images.shape[2:])\n",
    "    train_labels = np.array(train_labels, dtype=np.uint8)\n",
    "    train_labels = train_labels.reshape(-1, *train_labels.shape[2:])\n",
    "    print(f'train_images.shape: {train_images.shape}')\n",
    "    print(f'train_labels.shape: {train_labels.shape}')\n",
    "    \n",
    "    \n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    test_names = []\n",
    "    for images, labels, names in get_test_batch(1):\n",
    "        test_images.append(images)\n",
    "        test_labels.append(labels)\n",
    "        test_names += names\n",
    "\n",
    "    test_images = np.array(test_images, dtype=np.uint8)\n",
    "    test_images = test_images.reshape(-1, *test_images.shape[2:])\n",
    "    test_labels = np.array(test_labels, dtype=np.uint8)\n",
    "    test_labels = test_labels.reshape(-1, *test_labels.shape[2:])   \n",
    "    print(f'test_images.shape: {test_images.shape}')\n",
    "    print(f'test_labels.shape: {test_labels.shape}')\n",
    "    \n",
    "    flat_labels_size = reduce(lambda x, y: x*y, test_labels.shape[:-1])\n",
    "    image_org_shape = (test_labels.shape[1], test_labels.shape[2])\n",
    "    flat_offset = BATCH_SIZE*image_org_shape[0]*image_org_shape[1]\n",
    "    \n",
    "    image_input = tf.placeholder(tf.float32, (None, None, None, 3), name='image_input')\n",
    "    label_input = tf.placeholder(tf.int32, [None, None, None, NUM_CLASSES], name='label_input')\n",
    "    loss_weights = tf.placeholder(tf.float32, (None), name='loss_weights')\n",
    "    keep_prob = tf.placeholder_with_default(tf.constant(1.0, dtype=tf.float32), shape=(), name='keep_prob')\n",
    "    l_rate = tf.placeholder(tf.float32, name='l_rate')\n",
    "    adam_eps = tf.placeholder(tf.float32, name='adam_eps')\n",
    "    \n",
    "    logits, prediction, one_hot = network(image_input, keep_prob, WEIGHTS_FILE, NUM_CLASSES)\n",
    "    opt, total_loss = optimize(logits, label_input, l_rate, adam_eps, loss_weights)\n",
    "    \n",
    "    fscore_avg = 0.0\n",
    "    best_fscore = 0.0 # used to save model when fscore_avg increases\n",
    "    best_loss = 9999\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        print(f'\\nTraining epoch: {epoch+1}/{EPOCHS}')\n",
    "        \n",
    "        ''' \n",
    "        # Used to load images from disk, instead of memory\n",
    "        # Slower but necessary for larger datasets\n",
    "        # Not needed as swapfiles were used to increase functional RAM\n",
    "        for train_image_batch, train_label_batch, _ in get_train_batch(BATCH_SIZE):\n",
    "            \n",
    "            if FLIP:\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    # horizontal flip\n",
    "                    train_image_batch = np.flip(train_image_batch, axis=2)\n",
    "                    train_label_batch = np.flip(train_label_batch, axis=2)\n",
    "                \n",
    "            _, loss = sess.run([opt, total_loss],\n",
    "                               feed_dict = {image_input: train_image_batch,\n",
    "                                            label_input: train_label_batch,\n",
    "                                            keep_prob: KEEP_PROB,\n",
    "                                            l_rate: LEARNING_RATE})\n",
    "        print(f'Training time: {(time.time() - start_time):#0.3f}s, loss: {loss:#0.5f}') \n",
    "        '''\n",
    "        \n",
    "        if SHUFFLE_PER_EPOCH:\n",
    "            train_images, train_labels, train_names = shuffle(train_images, train_labels, train_names)\n",
    "            \n",
    "        for offset in range(0, len(train_images), BATCH_SIZE):\n",
    "            train_image_batch = train_images[offset:offset+BATCH_SIZE]\n",
    "            train_label_batch = train_labels[offset:offset+BATCH_SIZE]\n",
    "            \n",
    "            if FLIP:\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    # horizontal flip\n",
    "                    train_image_batch = np.flip(train_image_batch, axis=2)\n",
    "                    train_label_batch = np.flip(train_label_batch, axis=2)\n",
    "                \n",
    "            _, loss = sess.run([opt, total_loss],\n",
    "                               feed_dict = {image_input: train_image_batch,\n",
    "                                            label_input: train_label_batch,\n",
    "                                            loss_weights: LOSS_WEIGHTS,\n",
    "                                            keep_prob: KEEP_PROB,\n",
    "                                            l_rate: LEARNING_RATE,\n",
    "                                            adam_eps: ADAM_EPSILON})\n",
    "        print(f'Training time: {(time.time() - start_time):#0.3f}s, loss: {loss:#0.5f}')\n",
    "         \n",
    "        \n",
    "        sess_time = 0\n",
    "        total_preds = np.empty((flat_labels_size,), dtype=np.uint8)\n",
    "        total_labels = np.empty((flat_labels_size,), dtype=np.uint8)\n",
    "        for offset in range(0, len(test_images), BATCH_SIZE):\n",
    "            pred_time = time.time()\n",
    "            test_image_batch = test_images[offset:offset+BATCH_SIZE]\n",
    "            test_label_batch = test_labels[offset:offset+BATCH_SIZE]            \n",
    "            preds = sess.run(prediction, feed_dict = {image_input: test_image_batch})\n",
    "            \n",
    "            preds = revert_trim_reshape(preds)\n",
    "            sess_time += time.time() - pred_time\n",
    "            \n",
    "            preds_result = np.array(preds, dtype=np.uint8).reshape(-1)\n",
    "            labels_result = test_label_batch.argmax(axis=3).reshape(-1)\n",
    "            \n",
    "            batch_offset = len(test_label_batch)*image_org_shape[0]*image_org_shape[1]\n",
    "            i = int(offset/BATCH_SIZE)\n",
    "            total_preds[i*flat_offset:i*flat_offset+batch_offset] = preds_result\n",
    "            total_labels[i*flat_offset:i*flat_offset+batch_offset] = labels_result\n",
    "            \n",
    "        print(f'Prediction session time: {sess_time:#0.3f}s')\n",
    "        metrics = precision_recall_fscore_support(total_labels, total_preds)\n",
    "        del total_preds\n",
    "        del total_labels \n",
    "        \n",
    "        f1_str_1 = f'F1 scores: '\n",
    "        f1_str_2 = f'         '\n",
    "        for i, val in enumerate(metrics[2]):\n",
    "            f1_str_1 += f'{CHANNEL_NAMES[i]:8}'\n",
    "            f1_str_2 += f'{val:#8.4f}'\n",
    "        print(f1_str_1)\n",
    "        print(f1_str_2)\n",
    "        \n",
    "        prec_v = metrics[0][1]\n",
    "        prec_r = metrics[0][2]\n",
    "        recall_v = metrics[1][1]\n",
    "        recall_r = metrics[1][2]\n",
    "        if (prec_v==0 and recall_v==0) or (prec_r==0 and recall_r==0):\n",
    "            fscore_avg = 1e-6\n",
    "            print(f'NaN: division by zero in fscore_avg')\n",
    "        else:\n",
    "            fscore_v = 5 * (prec_v * recall_v) / (4 * prec_v + recall_v)\n",
    "            fscore_r = 1.25 * (prec_r * recall_r) / (0.25 * prec_r + recall_r)\n",
    "            fscore_avg = (fscore_v + fscore_r) / 2\n",
    "            print(f'prec_v: {prec_v:#0.5f}, recall_v: {recall_v:#0.5f}')\n",
    "            print(f'prec_r: {prec_r:#0.5f}, recall_r: {recall_r:#0.5f}')\n",
    "            print(f'fscore_avg: {fscore_avg:#0.5f}, fscore_v: {fscore_v:#0.5f}, fscore_r: {fscore_r:#0.5f}')\n",
    "        print(f'Total time: {time.time()-start_time:#0.3f}s')\n",
    "        \n",
    "        if fscore_avg - best_fscore > SAVE_EPSILON:\n",
    "            best_fscore = fscore_avg\n",
    "            saver.save(sess, os.path.join(SAVE_DIR, 'score', MODEL_NAME + '.ckpt'))  \n",
    "            print('*************** MODEL SAVED ON SCORE ***************')\n",
    "        elif best_loss - loss > SAVE_EPSILON:\n",
    "            best_loss = loss\n",
    "            saver.save(sess, os.path.join(SAVE_DIR, 'loss', MODEL_NAME + '.ckpt'))  \n",
    "            print('*** model saved on loss ***')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
