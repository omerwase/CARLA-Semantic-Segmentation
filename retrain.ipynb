{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Saved Models\n",
    "\n",
    "#### With slight modifications:\n",
    "    1) Lower learning rates\n",
    "    2) Increased Adam epsilon values\n",
    "    \n",
    "#### Results from final submitted run (after retraining):\n",
    "    MODEL_NAME: dcn_v5\n",
    "    MODEL_SAVE_VER: 00_r2\n",
    "    TRAIN_DIR: /home/ow/Documents/udacity/lyft/datasets/combined_v05/train_v4\n",
    "    TEST_DIR: /home/ow/Documents/udacity/lyft/datasets/combined_v05/test_v6\n",
    "    train_images.shape: (6400, 408, 800, 3)\n",
    "    train_labels.shape: (6400, 408, 800, 10)\n",
    "    test_images.shape: (500, 408, 800, 3)\n",
    "    test_labels.shape: (500, 600, 800, 10)\n",
    "\n",
    "    Training epoch: 12/200\n",
    "    Training time: 767.930s, loss: 0.01681\n",
    "    Prediction session time: 16.900s\n",
    "    F1 scores: Back   Vehi   Road   Fence  Ped    Poles  Side   Veg    BW     OT      \n",
    "               0.9549 0.7994 0.9924 0.7919 0.7320 0.7993 0.9660 0.8505 0.9090 0.7948\n",
    "    prec_v: 0.71346, recall_v: 0.90887\n",
    "    prec_r: 0.99116, recall_r: 0.99367\n",
    "    fscore_avg: 0.92666, fscore_v: 0.86167, fscore_r: 0.99166\n",
    "    Total time: 833.581s\n",
    "    *************** MODEL SAVED ON SCORE ***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import helper_functions as hf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "from functools import reduce\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'dcn_v5'\n",
    "MODEL_RESTORE_VER = '00_r1'\n",
    "MODEL_SAVE_VER = '00_r2'\n",
    "best_fscore = 0.90862\n",
    "SAVE_EPSILON = 1e-4\n",
    "\n",
    "EPOCHS = 200\n",
    "SHUFFLE_PER_EPOCH = True\n",
    "BATCH_SIZE = 12\n",
    "L2_REG = 1e-5\n",
    "STD_DEV = 1e-2\n",
    "LEARNING_RATE = 5e-6\n",
    "KEEP_PROB = 0.5 \n",
    "EPSILON = 1e-6\n",
    "ADAM_EPSILON = 1e-3\n",
    "\n",
    "TRIM_IND = (115, 523)\n",
    "FLIP = True\n",
    "RESHAPE = False\n",
    "PREPROCESS = True\n",
    "\n",
    "NEW_LABELS = True\n",
    "LABEL_CHANNELS = [10, 7, 2, 4, 5, 8, 9, 20, 30]\n",
    "CHANNEL_NAMES = ['Back', 'Vehi', 'Road', 'Fence', 'Ped', 'Poles', 'Side', 'Veg', 'BW', 'OT']\n",
    "LOSS_WEIGHTS = [0.3, 1.2, 0.4, 0.3, 1.0, 0.5, 0.3, 0.3, 0.3, 0.5]\n",
    "\n",
    "NUM_CLASSES = len(LABEL_CHANNELS) + 1\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'datasets', 'combined_v05')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train_v4')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test_v6')\n",
    "\n",
    "SAVE_DIR = os.path.join(os.getcwd(), 'saved_models', MODEL_NAME, MODEL_SAVE_VER)\n",
    "RESTORE_DIR = os.path.join(os.getcwd(), 'saved_models', MODEL_NAME, MODEL_RESTORE_VER, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Loading datasets ...')\n",
    "\n",
    "get_train_batch = hf.train_batch_gen(TRAIN_DIR, LABEL_CHANNELS, reshape=RESHAPE, \n",
    "                                     preprocess=PREPROCESS, new_labels=NEW_LABELS, \n",
    "                                     trim_ind=TRIM_IND)\n",
    "get_test_batch, revert_trim_reshape = hf.test_batch_gen(TEST_DIR, LABEL_CHANNELS, \n",
    "                                      reshape=RESHAPE, preprocess=PREPROCESS, new_labels=NEW_LABELS,\n",
    "                                      trim_ind=TRIM_IND)\n",
    "data_load_start = time.time()\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for images, labels, _ in get_train_batch(100):\n",
    "    train_images.append(images)\n",
    "    train_labels.append(labels)\n",
    "\n",
    "train_images = np.array(train_images, dtype=np.uint8)\n",
    "train_images = train_images.reshape(-1, *train_images.shape[2:])\n",
    "train_labels = np.array(train_labels, dtype=np.uint8)\n",
    "train_labels = train_labels.reshape(-1, *train_labels.shape[2:])\n",
    "\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for images, labels, _ in get_test_batch(100):\n",
    "    test_images.append(images)\n",
    "    test_labels.append(labels)\n",
    "    \n",
    "test_images = np.array(test_images, dtype=np.uint8)\n",
    "test_images = test_images.reshape(-1, *test_images.shape[2:])\n",
    "test_labels = np.array(test_labels, dtype=np.uint8)\n",
    "test_labels = test_labels.reshape(-1, *test_labels.shape[2:])   \n",
    "\n",
    "\n",
    "flat_labels_size = reduce(lambda x, y: x*y, test_labels.shape[:-1])\n",
    "image_org_shape = (test_labels.shape[1], test_labels.shape[2])\n",
    "flat_offset = BATCH_SIZE*image_org_shape[0]*image_org_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'MODEL_NAME: {MODEL_NAME}')\n",
    "print(f'MODEL_RESTORE_VER: {MODEL_RESTORE_VER}')\n",
    "print(f'MODEL_SAVE_VER: {MODEL_SAVE_VER}')\n",
    "print(f'TRAIN_DIR: {TRAIN_DIR}')\n",
    "print(f'TEST_DIR: {TEST_DIR}')\n",
    "print(f'train_images.shape: {train_images.shape}')\n",
    "print(f'train_labels.shape: {train_labels.shape}')\n",
    "print(f'test_images.shape: {test_images.shape}')\n",
    "print(f'test_labels.shape: {test_labels.shape}')\n",
    "print(f'Data load time: {time.time() - data_load_start:#0.1f}s')\n",
    "\n",
    "saver = tf.train.import_meta_graph(os.path.join(RESTORE_DIR, MODEL_NAME + '.ckpt.meta'))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(RESTORE_DIR))\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "    label_input = graph.get_tensor_by_name('label_input:0')\n",
    "    loss_weights = graph.get_tensor_by_name('loss_weights:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    l_rate = graph.get_tensor_by_name('l_rate:0')\n",
    "    adam_eps = graph.get_tensor_by_name('adam_eps:0')\n",
    "    prediction = graph.get_tensor_by_name('output/prediction:0')\n",
    "    total_loss = graph.get_tensor_by_name('optimize/total_loss:0')\n",
    "    opt = graph.get_operation_by_name('optimize/Adam')\n",
    "    \n",
    "    fscore_avg = 0.0\n",
    "    best_loss = 9999\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        print(f'\\nTraining epoch: {epoch+1}/{EPOCHS}')\n",
    "        \n",
    "        if SHUFFLE_PER_EPOCH:\n",
    "            train_images, train_labels = shuffle(train_images, train_labels)\n",
    "            \n",
    "        for offset in range(0, len(train_images), BATCH_SIZE):\n",
    "            train_image_batch = train_images[offset:offset+BATCH_SIZE]\n",
    "            train_label_batch = train_labels[offset:offset+BATCH_SIZE]\n",
    "            \n",
    "            if FLIP:\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    # horizontal flip\n",
    "                    train_image_batch = np.flip(train_image_batch, axis=2)\n",
    "                    train_label_batch = np.flip(train_label_batch, axis=2)\n",
    "                \n",
    "            _, loss = sess.run([opt, total_loss],\n",
    "                               feed_dict = {image_input: train_image_batch,\n",
    "                                            label_input: train_label_batch,\n",
    "                                            loss_weights: LOSS_WEIGHTS,\n",
    "                                            keep_prob: KEEP_PROB,\n",
    "                                            l_rate: LEARNING_RATE,\n",
    "                                            adam_eps: ADAM_EPSILON})\n",
    "            \n",
    "        print(f'Training time: {(time.time() - start_time):#0.1f}s, loss: {loss:#0.5f}')\n",
    "            \n",
    "        sess_time = 0\n",
    "        total_preds = np.empty((flat_labels_size,), dtype=np.uint8)\n",
    "        total_labels = np.empty((flat_labels_size,), dtype=np.uint8)\n",
    "        for offset in range(0, len(test_images), BATCH_SIZE):\n",
    "            pred_time = time.time()\n",
    "            test_image_batch = test_images[offset:offset+BATCH_SIZE]\n",
    "            test_label_batch = test_labels[offset:offset+BATCH_SIZE]            \n",
    "            preds = sess.run(prediction, feed_dict = {image_input: test_image_batch,\n",
    "                                                     keep_prob: 1.0})\n",
    "            \n",
    "            preds = revert_trim_reshape(preds)\n",
    "            sess_time += time.time() - pred_time\n",
    "            \n",
    "            preds_result = np.array(preds, dtype=np.uint8).reshape(-1)\n",
    "            labels_result = test_label_batch.argmax(axis=3).reshape(-1)\n",
    "            \n",
    "            batch_offset = len(test_label_batch)*image_org_shape[0]*image_org_shape[1]\n",
    "            i = int(offset/BATCH_SIZE)\n",
    "            total_preds[i*flat_offset:i*flat_offset+batch_offset] = preds_result\n",
    "            total_labels[i*flat_offset:i*flat_offset+batch_offset] = labels_result\n",
    "            \n",
    "        print(f'Prediction session time: {sess_time:#0.1f}s')\n",
    "        metrics = precision_recall_fscore_support(total_labels, total_preds)\n",
    "        del total_preds\n",
    "        del total_labels \n",
    "        \n",
    "        f1_str_1 = f'F1 scores: '\n",
    "        f1_str_2 = f'         '\n",
    "        for i, val in enumerate(metrics[2]):\n",
    "            f1_str_1 += f'{CHANNEL_NAMES[i]:8}'\n",
    "            f1_str_2 += f'{val:#8.4f}'\n",
    "        print(f1_str_1)\n",
    "        print(f1_str_2)\n",
    "        \n",
    "        prec_v = metrics[0][1]\n",
    "        prec_r = metrics[0][2]\n",
    "        recall_v = metrics[1][1]\n",
    "        recall_r = metrics[1][2]\n",
    "        if (prec_v==0 and recall_v==0) or (prec_r==0 and recall_r==0):\n",
    "            fscore_avg = 1e-6\n",
    "            print(f'NaN: division by zero in fscore_avg')\n",
    "        else:\n",
    "            fscore_v = 5 * (prec_v * recall_v) / (4 * prec_v + recall_v)\n",
    "            fscore_r = 1.25 * (prec_r * recall_r) / (0.25 * prec_r + recall_r)\n",
    "            fscore_avg = (fscore_v + fscore_r) / 2\n",
    "            print(f'prec_v: {prec_v:#0.5f}, recall_v: {recall_v:#0.5f}')\n",
    "            print(f'prec_r: {prec_r:#0.5f}, recall_r: {recall_r:#0.5f}')\n",
    "            print(f'fscore_avg: {fscore_avg:#0.5f}, fscore_v: {fscore_v:#0.5f}, fscore_r: {fscore_r:#0.5f}')\n",
    "        print(f'Total time: {time.time()-start_time:#0.1f}s')\n",
    "        \n",
    "        if fscore_avg - best_fscore > SAVE_EPSILON:\n",
    "            best_fscore = fscore_avg\n",
    "            saver.save(sess, os.path.join(SAVE_DIR, 'score', MODEL_NAME + '.ckpt'))  \n",
    "            print('*************** MODEL SAVED ON SCORE ***************')\n",
    "        elif best_loss - loss > SAVE_EPSILON:\n",
    "            best_loss = loss\n",
    "            saver.save(sess, os.path.join(SAVE_DIR, 'loss', MODEL_NAME + '.ckpt'))  \n",
    "            print('*** model saved on loss ***')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
